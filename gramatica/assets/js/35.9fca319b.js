(window.webpackJsonp=window.webpackJsonp||[]).push([[35],{338:function(t,n,a){"use strict";a.r(n);var e=a(15),s=Object(e.a)({},(function(){var t=this,n=t._self._c;return n("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[n("h1",{attrs:{id:"composition"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#composition"}},[t._v("#")]),t._v(" Composition")]),t._v(" "),n("p",[t._v("Sometimes instead of saying something like "),n("em",[t._v("That woman is "),n("strong",[t._v("beautiful")])]),t._v(" we will say "),n("em",[t._v("She is a "),n("strong",[t._v("beautiful woman")])]),t._v(".")]),t._v(" "),n("table",[n("thead",[n("tr",[n("th",[t._v("Tupi")]),t._v(" "),n("th",[t._v("English")]),t._v(" "),n("th",[t._v("Explanation")])])]),t._v(" "),n("tbody",[n("tr",[n("td",[n("root",{attrs:{root:"kunhã"}}),t._v(" "),n("root",{attrs:{root:"i",entryNumber:"4"}}),t._v(" "),n("root",{attrs:{root:"poranga"}})],1),t._v(" "),n("td",[t._v("(that) woman is beautiful")]),t._v(" "),n("td",[t._v("using the "),n("em",[t._v('"adjective"')]),t._v(" as a second class verb")])]),t._v(" "),n("tr",[n("td",[n("root",{attrs:{root:"i",entryNumber:"4"}}),t._v(" "),n("root",{attrs:{root:"kunhã"}}),n("root",{attrs:{type:"noun",root:"poranga"}})],1),t._v(" "),n("td",[t._v("She is a beautiful woman")]),t._v(" "),n("td",[t._v("using the "),n("em",[t._v('"adjective"')]),t._v(" in composition with the base noun to create a "),n("em",[t._v("new root")])])])])]),t._v(" "),n("p",[t._v("There are some rules to compositions which we need to take into account")]),t._v(" "),n("p",[t._v('def compose(self, modifier):\nframe = inspect.currentframe()\nfunc_name = frame.f_code.co_name\nargs, _, _, values = inspect.getargvalues(frame)\nargs_str = \', \'.join(f"{arg}={repr(values[arg])}" for arg in args if \'self\' != arg)\nret_noun = copy.deepcopy(self)\nret_noun.aglutinantes[-1] = self\nvbt = ret_noun.verbete()\nvbt_an = ret_noun.verbete(anotated=True)\nmod_vbt = modifier.verbete()\nmod_vbt_an = modifier.verbete(anotated=True)\n# Define some useful groups\nvogais_orais = "á e é i í y ý o ó u ú".split(" ")\nvogais_nasais =  "ã ẽ ĩ ỹ õ ũ".split(" ")\nnasais = "m n ng nh mb nd".split(" ")\nconsoantes = "p b t s k r gû û î ŷ".split(" ")')]),t._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",[n("code",[t._v('    if ends_with_any(vbt, vogais_orais):\n        parts = ret_noun.latest_verbete.split("[")\n        start = "[".join(parts[:-1])\n        start = self.remove_accent_last_vowel(start)\n        ret_noun.latest_verbete = f"{start}[{parts[-1]}{mod_vbt_an}"\n    elif ends_with_any(vbt, nasais) and starts_with_any(mod_vbt, vogais_orais+vogais_nasais):\n        parts = ret_noun.latest_verbete.split("[")\n        start = "[".join(parts[:-1])\n        ret_noun.latest_verbete = f"{start}[{parts[-1]}{mod_vbt_an}"\n    elif ends_with_any(vbt, nasais+consoantes) and starts_with_any(mod_vbt, ["\'"]):\n        parts = ret_noun.latest_verbete.split("[")\n        start = "[".join(parts[:-1])\n        ret_noun.latest_verbete = f"{start}[{parts[-1]}{mod_vbt_an[1:]}"\n    elif ends_with_any(vbt, nasais) and starts_with_any(mod_vbt, consoantes+nasais):\n        parts = ret_noun.latest_verbete.split("[")\n        start = "[".join(parts[:-1])\n        semivogal = \'\' if start[-2:].lower() != \'nh\' else \'î\'\n        start = remove_ending_if_any(start, nasais)\n        second_last_letter = self.nasal_map.get(start[-1], start[-1])\n        first_nasal = self.nasal_prefix_map.get(mod_vbt_an[0], mod_vbt_an[0]) if not self.is_nasal(mod_vbt) else mod_vbt_an[0]\n        ret_noun.latest_verbete = f"{start[:-1]}{second_last_letter}{semivogal}[{parts[-1]}{first_nasal}{mod_vbt_an[1:]}"\n    elif ends_with_any(vbt, vogais_nasais):\n        parts = ret_noun.latest_verbete.split("[")\n        start = "[".join(parts[:-1])\n        second_last_letter = self.nasal_map.get(start[-2], start[-2])\n        first_nasal = self.nasal_prefix_map.get(mod_vbt_an[0], mod_vbt_an[0]) if not self.is_nasal(mod_vbt) else mod_vbt_an[0]\n        ret_noun.latest_verbete = f"{start}[{parts[-1]}{first_nasal}{mod_vbt_an[1:]}"\n    elif ends_with_any(vbt, consoantes) and starts_with_any(mod_vbt, vogais_orais+vogais_nasais):\n        parts = ret_noun.latest_verbete.split("[")\n        start = "[".join(parts[:-1])\n        ret_noun.latest_verbete = f"{start}[{parts[-1]}{mod_vbt_an}"\n    elif ends_with_any(vbt, consoantes) and starts_with_any(mod_vbt, consoantes):\n        parts = ret_noun.latest_verbete.split("[")\n        start = "[".join(parts[:-1])\n        start = remove_ending_if_any(start, consoantes)\n        ret_noun.latest_verbete = f"{start}[{parts[-1]}{mod_vbt_an}"\n\n    ret_noun.aglutinantes.append(ret_noun)\n    ret_noun.recreate += f".{func_name}({args_str})"\n    return ret_noun')])])])])}),[],!1,null,null,null);n.default=s.exports}}]);